{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_TensorFlow.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MWHhxtXEd7S",
        "colab_type": "code",
        "outputId": "3ad0938c-b745-46c7-f652-47c8081ebeb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf1\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-c00bece8f7db>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi5SzNHAE3eV",
        "colab_type": "code",
        "outputId": "1467dcdf-b2e0-45e4-e7b9-6aab58bdb552",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        }
      },
      "source": [
        "## Outline the model\n",
        "input_size = 784\n",
        "output_size = 10\n",
        "hidden_layer_size = 200\n",
        "tf.reset_default_graph()\n",
        "\n",
        "## Declare Placeholder\n",
        "inputs= tf1.placeholder(tf.float32, [None,input_size])\n",
        "targets= tf1.placeholder(tf.float32,[None,output_size])\n",
        "\n",
        "## Weights and biases for first linear combination\n",
        "weights_1 = tf1.get_variable('weights_1',[input_size, hidden_layer_size])\n",
        "biases_1 = tf1.get_variable('biases_1',[hidden_layer_size])\n",
        "\n",
        "##Operation between input & first hidden layer using Relu activation function\n",
        "output_1 = tf.nn.relu(tf.matmul(inputs,weights_1)+biases_1)\n",
        "\n",
        "##Weights and biases for second hidden layer\n",
        "weights_2 = tf1.get_variable('weights_2',[hidden_layer_size,hidden_layer_size])\n",
        "biases_2 = tf1.get_variable('biases_2',[hidden_layer_size])\n",
        "\n",
        "##Operation between first and second hidden layer usinf Relu activation function\n",
        "output_2 = tf.nn.relu(tf.matmul(output_1,weights_2)+biases_2)\n",
        "\n",
        "##Weight and biases for second hidden layer and output\n",
        "weights_3 = tf1.get_variable('weights_3',[hidden_layer_size, output_size])\n",
        "biases_3 = tf1.get_variable('biases',[output_size])\n",
        "\n",
        "## Operation between second hidden layer and output\n",
        "outputs= tf.matmul(output_2,weights_3)+biases_3\n",
        "\n",
        "## Calculating loss by applying softmax at the last layer\n",
        "loss= tf.nn.softmax_cross_entropy_with_logits(logits= outputs, labels=targets)\n",
        "\n",
        "##Get average loss\n",
        "mean_loss= tf.reduce_mean(loss)\n",
        "\n",
        "## Using ADAM Optimizer\n",
        "optimize = tf.train.AdamOptimizer(learning_rate=0.001).minimize(mean_loss)\n",
        "\n",
        "##Check whether output is equal or not for all 10 outputs\n",
        "out_equals_target = tf.equal(tf.argmax(outputs,1),tf.argmax(targets,1))\n",
        "\n",
        "##Average accuracy of output\n",
        "accuracy = tf.reduce_mean(tf.cast(out_equals_target,tf.float32))\n",
        "\n",
        "##Declaration of session variable\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "##Initialize variable\n",
        "initializer = tf1.global_variables_initializer()\n",
        "sess.run(initializer)\n",
        "\n",
        "##Batching\n",
        "batch_size= 500\n",
        "\n",
        "##Batches per epoch\n",
        "batches_number = int(mnist.train.num_examples/batch_size)\n",
        "\n",
        "##Basic early stopping \n",
        "max_epochs = 15\n",
        "\n",
        "## Set validation loss to trigger early stopping\n",
        "prev_validation_loss = 9999999.\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "## Loop for each epoch\n",
        "for epoch_counter in range(max_epochs):\n",
        "  curr_epoch_loss = 0\n",
        "  for batch_counter in range(batches_number):\n",
        "    input_batch, target_batch = mnist.train.next_batch(batch_size)\n",
        "    _, batch_loss = sess.run([optimize,mean_loss],\n",
        "                             feed_dict={inputs: input_batch, targets:target_batch})\n",
        "    curr_epoch_loss += batch_loss\n",
        "  ## Average batch loss\n",
        "  curr_epoch_loss /= batches_number\n",
        "  input_batch, target_batch = mnist.validation.next_batch(mnist.validation._num_examples)\n",
        "\n",
        "  ## Run without the optimization step (simply forward propagate)\n",
        "  validation_loss, validation_accuracy = sess.run([mean_loss, accuracy], \n",
        "                                                  feed_dict={inputs: input_batch, targets: target_batch})\n",
        "    \n",
        "    \n",
        "  ## Print statistics for the current epoch\n",
        "  print('Epoch '+str(epoch_counter+1)+\n",
        "        '. Mean loss: '+'{0:.3f}'.format(curr_epoch_loss)+\n",
        "        '. Validation loss: '+'{0:.3f}'.format(validation_loss)+\n",
        "        '. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.)+'%')\n",
        "  \n",
        "  ## Trigger early stopping if validation loss begins increasing.\n",
        "  if validation_loss > prev_validation_loss:\n",
        "      break\n",
        "      \n",
        "  ## Store this epoch's validation loss to be used as previous validation loss in the next iteration.\n",
        "  prev_validation_loss = validation_loss\n",
        "\n",
        "  print('End of training.')\n",
        "\n",
        "  ## The time it took the algorithm to train\n",
        "  print(\"Training time: %s seconds\" % (time.time() - start_time))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1. Mean loss: 0.479. Validation loss: 0.195. Validation accuracy: 94.76%\n",
            "End of training.\n",
            "Training time: 1.6533458232879639 seconds\n",
            "Epoch 2. Mean loss: 0.174. Validation loss: 0.135. Validation accuracy: 96.52%\n",
            "End of training.\n",
            "Training time: 3.2629518508911133 seconds\n",
            "Epoch 3. Mean loss: 0.122. Validation loss: 0.115. Validation accuracy: 96.76%\n",
            "End of training.\n",
            "Training time: 4.861364126205444 seconds\n",
            "Epoch 4. Mean loss: 0.095. Validation loss: 0.094. Validation accuracy: 97.28%\n",
            "End of training.\n",
            "Training time: 6.55009913444519 seconds\n",
            "Epoch 5. Mean loss: 0.073. Validation loss: 0.083. Validation accuracy: 97.38%\n",
            "End of training.\n",
            "Training time: 8.223925113677979 seconds\n",
            "Epoch 6. Mean loss: 0.058. Validation loss: 0.078. Validation accuracy: 97.50%\n",
            "End of training.\n",
            "Training time: 9.902164459228516 seconds\n",
            "Epoch 7. Mean loss: 0.049. Validation loss: 0.076. Validation accuracy: 97.62%\n",
            "End of training.\n",
            "Training time: 11.596010446548462 seconds\n",
            "Epoch 8. Mean loss: 0.040. Validation loss: 0.075. Validation accuracy: 97.62%\n",
            "End of training.\n",
            "Training time: 13.255913972854614 seconds\n",
            "Epoch 9. Mean loss: 0.033. Validation loss: 0.071. Validation accuracy: 97.78%\n",
            "End of training.\n",
            "Training time: 14.922059297561646 seconds\n",
            "Epoch 10. Mean loss: 0.026. Validation loss: 0.069. Validation accuracy: 98.02%\n",
            "End of training.\n",
            "Training time: 16.5877583026886 seconds\n",
            "Epoch 11. Mean loss: 0.021. Validation loss: 0.081. Validation accuracy: 97.58%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxxmVLJiMAJd",
        "colab_type": "code",
        "outputId": "d36228f1-ee03-4f9f-c7d4-bf2d5fdad305",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Test the model\n",
        "input_batch, target_batch = mnist.test.next_batch(mnist.test._num_examples)\n",
        "test_accuracy = sess.run([accuracy], \n",
        "    feed_dict={inputs: input_batch, targets: target_batch})\n",
        "\n",
        "\n",
        "## print (test_accuracy)\n",
        "test_accuracy_percent = test_accuracy[0] * 100.\n",
        "\n",
        "## Print the test accuracy formatted in percentages\n",
        "print('Test accuracy: '+'{0:.2f}'.format(test_accuracy_percent)+'%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 96.41%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe9MjX3-7KJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}